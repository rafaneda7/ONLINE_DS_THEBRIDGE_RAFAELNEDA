{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ8-spwGrtMI"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/cabecera.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43dkXdozrtMK"
      },
      "source": [
        "## Una (muy) breve (pero densa) introducción a la IA Generativa en Textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Tab-WKrtMK"
      },
      "source": [
        "*NOTA: Este notebook adapta y amplía parte del capítulo dedicado a NLP en el excelente libro [Hands on Machine Learning for Python](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#nlp_chapter)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-TYG-VJrtML"
      },
      "source": [
        "La generación de textos y más que eso el tratamiento de lenguaje natural ha dado un salto \"cuántico\" en los últimos años dentro del campo de la IA. Vamos a ver de una forma poco ortodoxa la evolución partiendo de las arquitecturas más complejas sobre redes recurrentes hasta terminar en los instruct LLM (la base de la IA multimodal generativa actual). Será un viaje denso y breve, pero espero que despierte el interés en ti para ampliar más con el material extra que se proporciona en la plataforma y en algunos enlaces de este notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTwE2Qu4rtMM"
      },
      "source": [
        "*Antes de empezar, este notebook \"no\" se recomienda si no se ejecuta en un entorno con GPU disponible. Si no es el caso -> Colab:*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUIn-3_qrtMM"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/rodolso/DS_Online_Octubre24/blob/main/05_Deep_Learning/Sprint_19/Unidad_02_IA_Generativa_NLP_y_Texto/Generativa_Texto_De_RRN_a_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XXGCfC43rtMN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rN3amKO8mBC"
      },
      "source": [
        "**Warning**: las versiones más recientes de TensorFlow están basadas en Keras 3. Para las sesiones previas no fue muy difícil actualizar el código para que fuera compatible con Keras 3, pero desafortunadamente es mucho más complicado para esta sesión (especialmente en la parte final de la misma). Por ello, nos vemos obligados a volver a Keras 2. Para hacer esto, vamos a configurar la variable de entorno `TF_USE_LEGACY_KERAS` a `\"1\"` e importamos el paquete `tf_keras`. Esto asegura que `tf.keras` apunte correctamente a `tf_keras`, que es Keras 2.*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a0b9iwIEzGKD"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if IS_COLAB:\n",
        "    import os\n",
        "    os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "    import tf_keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gCHANlswrtMO"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Rg025n4NrtMO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6pZ5hfvkrtMP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "from random import random, randint,sample\n",
        "from time import time, sleep\n",
        "\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oqYnbmiRrtMP"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
        "              \"accelerator.\")\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDnv_Sx0rtMP"
      },
      "source": [
        "## Usando RNNs: Una estructura Encoder-Decoder para traducir de inglés a español"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBcIBBUArtMP"
      },
      "source": [
        "El primer gran avance por encima de la vectorización y las técnicas iniciales de resumen (summarization), traducción (translation), preguntas y respuestas (Q&A), es el empleo de redes recurrentes en el tratamiento de textos. Veamos un ejemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiDAs2-VrtMP"
      },
      "source": [
        "El objetivo del siguiente ejemplo es doble:  \n",
        "1. Mostrar cómo las redes recurrentes pueden configurarse para tratar un problema de traducción donde a una secuencia de entrada de longitud variable le corresponderá una secuencia de longitud variable y además no necesariamente coincidente, en dicha longitud, con la de entrada. Este problema se aplica a la traducción de inglés a español pero es un esquema que se puede emplear en cualquier tipo de cambio de representación entre secuencias (yes, para pasar de una secuencia a una imagen y viceversa).   \n",
        "\n",
        "2. Introducir de forma progresiva el concepto de _Atention_ (atención) para mejorar el modelo anterior y sobre este la arquitectura conocida como Transformers, que nos permitirá hablar de GPT, BERT y los LLM (Large Language Models, no Master of Laws, ojo) en general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ81WAyHrtMQ"
      },
      "source": [
        "### El dataset de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh3YfbK_rtMQ"
      },
      "source": [
        "Utilizaremos un datset que empareja palabras y frases en inglés con sus traducciones al español de Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Kry1EyVWrtMQ"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\",\n",
        "                               origin=url,\n",
        "                               cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text(encoding = \"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emipPlQartMQ"
      },
      "source": [
        "Observamos su contenido (siempre hay que mirar la mercancía antes de ponerse a cocinar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PsOOYjNrtMQ",
        "outputId": "282d40fe-21a9-4189-dcb7-166bf355d9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVe.\n",
            "Go.\tVete.\n",
            "Go.\tVaya.\n",
            "Go.\tVáyase.\n",
            "Hi.\tHola.\n",
            "Run!\t¡Corre!\n",
            "Run.\tCorred.\n",
            "Who?\t¿Quién?\n",
            "Fire!\t¡Fueg\n"
          ]
        }
      ],
      "source": [
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34GsOC45rtMQ"
      },
      "source": [
        "Lo transformamos para que sea una relación de secuencia a secuencia (dada una secuencia tenemos su target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "o8hoHlQwrtMR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separa las parejas en dos listas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "hr6GoqdzuVq9",
        "outputId": "e1f7e39f-e433-47db-8efd-b6780042740b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tenemos 118964 sentencias para entrenar\n",
            "Distribuciones del corpus en inglés\n",
            "count    118964.000000\n",
            "mean          6.310363\n",
            "std           2.611586\n",
            "min           1.000000\n",
            "25%           4.000000\n",
            "50%           6.000000\n",
            "75%           8.000000\n",
            "max          47.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(f\"Tenemos {len(sentences_en)} sentencias para entrenar\")\n",
        "print(\"Distribuciones del corpus en inglés\")\n",
        "series_en = pd.Series([len(sentencia.split()) for sentencia in sentences_en])\n",
        "series_en.hist();\n",
        "print(series_en.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "pPbLpRYru4F3",
        "outputId": "bf0e9471-ac8d-4668-ca28-21f00424b04e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuciones del corpus en español\n",
            "count    118964.000000\n",
            "mean          6.083866\n",
            "std           2.764452\n",
            "min           1.000000\n",
            "25%           4.000000\n",
            "50%           6.000000\n",
            "75%           7.000000\n",
            "max          49.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Distribuciones del corpus en español\")\n",
        "series_es = pd.Series([len(sentencia.split()) for sentencia in sentences_es])\n",
        "series_es.hist();\n",
        "print(series_es.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wiqv5UaVrtMR",
        "outputId": "0fdbb3e4-bf71-4182-e31b-fe1d261bb30a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He has a natural gift for speaking.(7), '=>', Tiene un don natural para hablar.(6)\n",
            "I have lost my keys.(5), '=>', Perdí mis llaves.(3)\n",
            "Tom arrived this morning.(4), '=>', Tom llegó esta mañana.(4)\n"
          ]
        }
      ],
      "source": [
        "origen = randint(0,len(sentences_es)-3)\n",
        "for i in range(origen, origen+3):\n",
        "    print(f\"{sentences_en[i]}({len(sentences_en[i].split())}), '=>', {sentences_es[i]}({len(sentences_es[i].split())})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INrVVj9_rtMR"
      },
      "source": [
        "### ENCODER-DECODER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_pkKlb_rtMR"
      },
      "source": [
        "En la sesión de redes recurrentes ya vimos la estructura básica y citamos algún uso de la misma  \n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K6q_NP-rtMR"
      },
      "source": [
        "Y, ¿por qué esta arquitectura? Porque antes de que se propusiese no había forma de entrenar modelos que admitiesen secuencias de longitud variable recibiendo como target otra secuencia de longitud variable (y por tanto pudiendo ser esa longitud diferente a la primera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGjjpK26rtMR"
      },
      "source": [
        "El encoder ahora se encarga de convertir cualquier secuencia que haya a la entrada en un vector de longitud fija y el decoder convertira este vector en una secuencia de salida de longitud variable.  \n",
        "\n",
        "De hecho al encoder le vamos a dar de comer secuencias de longitud fija pero lo suficientemente larga como para que entren todas, y aplicaremos el truco del padding para completar y el de la máscara para que no le afecte a las secuencias cortas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__amabJsrtMS"
      },
      "source": [
        "Este es el modelo que vamos a construir (sin \"desenrrollar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLiJdIRZrtMS"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder_to_train.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYk-xYBOrtMS"
      },
      "source": [
        "Mejor si lo desenrrollamos:\n",
        "\n",
        "<img src=\"https://github.com/rodolso/DS_Online_Octubre24/blob/main/05_Deep_Learning/Sprint_19/Unidad_02_IA_Generativa_NLP_y_Texto/img/encoder_decoder_unrolled.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpCMaENGrtMS"
      },
      "source": [
        "Veamos como funcionaría (en entrenamiento) [*nota: h y c son los hidden_state de la recurrente del encoder, $h_d$ y $c_d$ son los hidden_state de la recurrente del decoder*]:\n",
        "1. Al enconder le damos la secuencia [I, like, soccer], y no le va a pasar nada todavía al encoder...\n",
        "2. Hace el embedding, supongamos que de 2 dimensiones, [ (0.212,-3.32), (1.34, 0.344), (6.665,-4.443)]\n",
        "3. Procesa la secuencia uno a uno y va transmitiéndose el hidden_state (y la cell_state, es una LSTM) en cada elemento de la secuencia:  \n",
        "    > Procesa e0: [(0.212,-3.32),(0,0,...0),(0,0.....)]  \n",
        "    > Procesa e1: [(1.34,0.344), h([(0.212,-3.32),(0,0,...0),(0,0.....)]),c((0.212,-3.32),(0,0,...0),(0,0.....))] (recordad que las LSTM tienen dos estados ocultos h y c el primero en teoría para la memoria a corto y el segundo para la memoria a largo)  \n",
        "    > Procesa e2 [(6.665, -4.443), h(e1), c(1)]  \n",
        "4. Ahora sí devuelve [salida(e2),h(e2),c(2)] y esto es parte de lo que entra en el Decoder\n",
        "5. El decoder a la vez ha hecho el embedding de su entrada [emb(\"\\<sos\\>\"),emb(\"Me\"),emb(\"gusta\"),emb(\"el\"),emb(\"fútbol\")]\n",
        "5. Lo primero que procesa el decoder es d1: [h(e2),c(e2),emb(\"\\<sos\\>\")] y la capa de salida predice (en el caso de la figura) \"me\"  \n",
        "6. Luego procesa d2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1)] y la capa de salida predice (en este caso): \"encanta\"\n",
        "7. procesa d3: [emb(\"gusta\"),$h_d$(d2),$c_d$(d2)] y la capa de salida predice: \"el\" (Importante, le entra el embedding de la palabra que tendría que haber predicho antes (\"gusta\") no la que realmente predijo \"encanta\", esto es *Teaching Forcing*)\n",
        "8. procesa d4: [emb(\"el\"),$h_d$(d3),$c_d$(d3)] y la capa de salida predice: \"fútbol\"\n",
        "9. procesa d5: [emb(\"fútbol\"),$h_d$(d4),$c_d$(4)] y la capa de salida predice: \"\\<eos\\>\" (end of sequence) (podría haber hecho la predicción de otra palabra y hubiera acabado igual pero se contabilizaría como un error para el optimizador, etc, etc)\n",
        "10. Se acaba la secuencia de entrada para el decoder\n",
        "\n",
        "A destacar:\n",
        "- El encoder sólo le pasara los hidden_state (h y c) del final de la secuencia de entrada al decoder\n",
        "- El decoder trabaja sobre el target completo desplazado una vez (esto nos sirve para construir el vec2seq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8ADOrEjrtMS"
      },
      "source": [
        "### Construcción del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-mpvJODrtMS"
      },
      "source": [
        "Ok, ahora que todo ha quedado clarito como la teoría de la relatividad, vamos a construir el modelo.  \n",
        "\n",
        "Primero las capas de embeddings: como ya hemos visto primero nuestros vectorizadores para convertir cada sentencia en secuencia de índices y después la capa de embedding para que aprenda cuál es la mejor reprensentación de cada índice/palabra en el contexto del problema que estamos resolviendo. (De hecho, ***inciso: ¿qué es lo que realmente está haciendo el encoder...***, *se te ocurre qué podríamos hacer con el encoder una vez entrenado todo el modelo...*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "luUMMVZ0rtMS"
      },
      "outputs": [],
      "source": [
        "vocab_size = 5000 # Número de tokens de nuestro vocabulario, en este caso vamos a hacer que token = (conjunto caracteres separados por espacios)\n",
        "max_length = 50 # Las secuencias de entrada están fijadas a 50, podríamos haberlas fijado a...\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length) # Como no decimos nada split=\"whitespace\", o sea la tokenizacion mencionada\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es]) # Importante le añadimos el comienzo de secuencia y el final para que sepa donde empieza y para que aprenda cuando se acaba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxiguIiTrtMT",
        "outputId": "62712792-650c-4b0e-973b-84ad8acade5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " np.str_('the'),\n",
              " np.str_('i'),\n",
              " np.str_('to'),\n",
              " np.str_('you'),\n",
              " np.str_('tom'),\n",
              " np.str_('a'),\n",
              " np.str_('is'),\n",
              " np.str_('he')]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRDoz0NOrtMT",
        "outputId": "f9284869-d804-4a86-85f2-24cdabdb9e43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " np.str_('startofseq'),\n",
              " np.str_('endofseq'),\n",
              " np.str_('de'),\n",
              " np.str_('que'),\n",
              " np.str_('a'),\n",
              " np.str_('no'),\n",
              " np.str_('tom'),\n",
              " np.str_('la')]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKXkImlH5Sdd"
      },
      "source": [
        "Veamos cómo codifica algunas de las setencias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH1Nz_po5iV4",
        "outputId": "e0d46ba9-485b-47f6-c0ea-43b030b4d974"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Isn't it a lovely morning?(5), '=>', No es una hermosa mañana?(5)\n",
            "Vectorizacion sin embedding de la entrada al encoder\n",
            "tf.Tensor(\n",
            "[ 127   13    7 2506  215    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n",
            "Vectorizacion sin embedding de la entrada al decoder\n",
            "tf.Tensor(\n",
            "[  2   7  12  18 780  81   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(50,), dtype=int64)\n",
            "Vectorizacion del target\n",
            "tf.Tensor(\n",
            "[  7  12  18 780  81   3   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(50,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "origen = randint(0,len(sentences_es)-3)\n",
        "for i in range(origen, origen+1):\n",
        "    print(f\"{sentences_en[i]}({len(sentences_en[i].split())}), '=>', {sentences_es[i]}({len(sentences_es[i].split())})\")\n",
        "    print(\"Vectorizacion sin embedding de la entrada al encoder\", text_vec_layer_en(sentences_en[i]), sep = \"\\n\")\n",
        "    print(\"Vectorizacion sin embedding de la entrada al decoder\", text_vec_layer_es(f\"startofseq {sentences_es[i]}\"), sep = \"\\n\")\n",
        "    print(\"Vectorizacion del target\", text_vec_layer_es(f\"{sentences_es[i]} endofseq\"), sep = \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGVInDEhrtMT"
      },
      "source": [
        "Construimos los datasets de entrenamiento y validación, teniendo en cuenta que encoder y decoder reciben entradas ligeramente diferentes y que el target debe contener el __endofseq__ (que es un token que debe predecir el modelo, es decir debe predecir cuando acaba la frase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6-vrcZDyrtMT"
      },
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6XplSzgrtMT"
      },
      "source": [
        "Y ahora sí, comenzamos con la definición (funcional del modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tB_pShS-rtMT"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KdCZZOiSrtMU"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "O6GgJ1xrrtMU"
      },
      "outputs": [],
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings) # IMPORTANTE obtenemos los estados de salida del encoder que es lo que...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "BIHHLh7prtMU"
      },
      "outputs": [],
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state) # ... realmente vamos a pasar al decoder para el primer token (<sos>) de la secuencia de guía (que en el entrenamiento es la de target desplazada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "FVaTrH1TrtMU"
      },
      "outputs": [],
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\") # La salida es una softmax con tantas neuronas como términos en el vocabulario, es decir estamos prediciendo el índice de cada palabra de la respuesta. Luego tendremos que decodificarlo para obtener la palabra real\n",
        "Y_proba = output_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-W99DourtMW"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZT7TWtjrtMW",
        "outputId": "1b1cdf46-9487-4ad1-8767-0a7ece82f782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 73s 19ms/step - loss: 3.9948 - accuracy: 0.3227 - val_loss: 3.0521 - val_accuracy: 0.4283\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 44s 14ms/step - loss: 2.5640 - accuracy: 0.4884 - val_loss: 2.3030 - val_accuracy: 0.5299\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 45s 14ms/step - loss: 1.8887 - accuracy: 0.5845 - val_loss: 1.9324 - val_accuracy: 0.5902\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 44s 14ms/step - loss: 1.4686 - accuracy: 0.6550 - val_loss: 1.7601 - val_accuracy: 0.6220\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 44s 14ms/step - loss: 1.1837 - accuracy: 0.7077 - val_loss: 1.6892 - val_accuracy: 0.6368\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 44s 14ms/step - loss: 0.9706 - accuracy: 0.7508 - val_loss: 1.6772 - val_accuracy: 0.6411\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 46s 15ms/step - loss: 0.8040 - accuracy: 0.7878 - val_loss: 1.6944 - val_accuracy: 0.6452\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 45s 15ms/step - loss: 0.6704 - accuracy: 0.8188 - val_loss: 1.7334 - val_accuracy: 0.6466\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 46s 15ms/step - loss: 0.5630 - accuracy: 0.8453 - val_loss: 1.7867 - val_accuracy: 0.6443\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 46s 15ms/step - loss: 0.4777 - accuracy: 0.8671 - val_loss: 1.8474 - val_accuracy: 0.6415\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7967cb44d310>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPtEaBMqrtMW"
      },
      "source": [
        "Una vez entrenado el modelo, la traducción tiene un poco de miga.   \n",
        "\n",
        "El decoder espera que le pasemos una secuencia guía (el teacher), que es la función que hacía la secuencia target desplazada uno en el entrenamiento.  \n",
        "\n",
        "Lo que vamos a hacer es ir prediciendo palabra a palabra introduciendo como guía la última predicción hasta llegar a que el modelo devuelva el carácter de fin de secuencia y en ese momento devolvemos la \"traducción\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "adTniqXUrtMW"
      },
      "outputs": [],
      "source": [
        "def translate(sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = np.array([sentence_en]).astype(object)  # encoder input\n",
        "        X_dec = np.array([\"startofseq \" + translation]).astype(object)  # decoder input\n",
        "        y_probs = model.predict((X, X_dec))\n",
        "        y_proba = y_probs[0, word_idx]  # last token's probas\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_proba = round(float(y_proba[predicted_word_id]),3)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "        translation += \" \" + predicted_word\n",
        "        print(f\"{translation}({predicted_proba})\")\n",
        "    return translation.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnh_m6khrtMW"
      },
      "source": [
        "Probemos con algo sencillo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "CKivIVBkrtMW",
        "outputId": "4c7ef2f5-969b-4538-ec50-02fce75d8bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            " me(0.996)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta(1.0)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta el(0.994)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta el fútbol(0.991)\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJfrl0mNeaS"
      },
      "source": [
        "Y si cambiamos un poco las palabras...:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "G27He7G0NdPh",
        "outputId": "4c6c5a20-f48d-4617-f428-d1b135109e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            " me(0.938)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me encanta(0.936)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me encanta jugar(0.975)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me encanta jugar al(0.984)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me encanta jugar al béisbol(0.578)\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me encanta jugar al béisbol'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "translate(\"I love playing football\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UU9czJjrtMW"
      },
      "source": [
        "Bien!!! Pero qué ocurre si le pedimos algo más largo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "sbpR3btrrtMX",
        "outputId": "987611f9-be66-40e8-930e-70d40486b57c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            " me(0.96)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta(0.999)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta la(0.38)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta la escuela(0.408)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta la escuela para(0.492)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta la escuela para no(0.251)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta la escuela para no [UNK](0.686)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta la escuela para no [UNK] el(0.613)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta la escuela para no [UNK] el [UNK](0.22)\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta la escuela para no [UNK] el [UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ov8c7UzrtMX"
      },
      "source": [
        "Vamos a ver mejoras que además nos vayan adelantando conceptos para llegar a los LLN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RreDpT8rtMX"
      },
      "source": [
        "## Bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zudtQcm1rtMX"
      },
      "source": [
        "Una red recurrente bidireccional es la que lee la secuencia tanto de izquierda a derecha como de derecha a izquierda y procesa ambas secuencias en conjunto. Ojo: la secuencia de entrada.\n",
        "\n",
        "En general es como tener una capa que mira en un sentido y otra en el otro y concatenar luego sus salidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSNm3wlTrtMX"
      },
      "source": [
        "<img src=\"https://github.com/rodolso/DS_Online_Octubre24/blob/main/05_Deep_Learning/Sprint_19/Unidad_02_IA_Generativa_NLP_y_Texto/img/bidirectionalrnn.jpg?raw=1\" alt=\"Bidirectional RNN\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzxTTMFzrtMX"
      },
      "source": [
        "¿Por qué y para qué? Porque, por ejemplo, hay frases que para traducirlas necesitas ver que viene después, como en el caso de los adjetivos en inglés que anteceden al nombre y de los sinónimos en un idioma que no coinciden necesariamente con los sinónimos en otro: the left arm, they left party, they left the restaurant...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kdGZV5FrtMX"
      },
      "source": [
        "Para crear un capa recurrente bidireccional, se hace lo siguiente (encapsular una recurrente en una más genérica denominada Bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "a4fFw2scrtMY"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrN_tlVyrtMY"
      },
      "source": [
        "En el caso del decoder no podemos hacer lo mismo porque en el target mirar al futuro sí es hacer trampa (recordemos que hasta ahora le pasamos la palabra que correspondería a la palabra esperada anterior, o sea no hacemos trampa), y por tanto no serviría para predecir algo que no hubiera visto (de hecho no podríamos construir una entrada para predecir de forma correcta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jle372OvrtMY"
      },
      "source": [
        "Pero, la recurrente bidireccional produce el doble de estados ocultos. Como se trata de una LSTM tendremos dos hidden_state y dos cell_state, aunque el decoder sólo espera dos (porque es otra LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6UyFduHrtMY"
      },
      "source": [
        "Lo que hacemos es concatenarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5dIrmT73V-BU"
      },
      "outputs": [],
      "source": [
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "# Wrap tf.concat within a Lambda layer to make it compatible with KerasTensors\n",
        "encoder_state = [tf.keras.layers.Lambda(lambda x: tf.concat(x[::2], axis=-1))(encoder_state),  # short-term (0 & 2)\n",
        "                 tf.keras.layers.Lambda(lambda x: tf.concat(x[1::2], axis=-1))(encoder_state)]  # long-term (1 & 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532Ve8sgrtMb"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZadV-ZArtMc",
        "outputId": "c1c4d625-1767-4ccf-b43c-e88b9c9724af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 89s 24ms/step - loss: 2.9388 - accuracy: 0.4710 - val_loss: 2.0222 - val_accuracy: 0.5852\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 55s 18ms/step - loss: 1.5773 - accuracy: 0.6476 - val_loss: 1.6168 - val_accuracy: 0.6446\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 55s 18ms/step - loss: 1.1722 - accuracy: 0.7155 - val_loss: 1.5009 - val_accuracy: 0.6664\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 57s 18ms/step - loss: 0.9367 - accuracy: 0.7608 - val_loss: 1.4803 - val_accuracy: 0.6727\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 57s 18ms/step - loss: 0.7666 - accuracy: 0.7970 - val_loss: 1.4971 - val_accuracy: 0.6744\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 56s 18ms/step - loss: 0.6343 - accuracy: 0.8278 - val_loss: 1.5437 - val_accuracy: 0.6707\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 57s 18ms/step - loss: 0.5317 - accuracy: 0.8523 - val_loss: 1.5936 - val_accuracy: 0.6712\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 57s 18ms/step - loss: 0.4505 - accuracy: 0.8730 - val_loss: 1.6645 - val_accuracy: 0.6690\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 57s 18ms/step - loss: 0.3870 - accuracy: 0.8897 - val_loss: 1.7281 - val_accuracy: 0.6663\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 57s 18ms/step - loss: 0.3379 - accuracy: 0.9026 - val_loss: 1.7927 - val_accuracy: 0.6626\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x796774184590>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# extra code — completes the model and trains it\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "qUG81j9YrtMc",
        "outputId": "1bab5b0f-0661-49e7-e9ad-3811f903b14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            " me(0.983)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta(1.0)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el(0.999)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta el [UNK](0.531)\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el [UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "-NF2SQovUKdb",
        "outputId": "0c032648-2331-4246-e0c6-efacc760a10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            " me(0.921)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta(0.993)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el(0.606)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol(0.339)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y(1.0)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y a(0.231)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " me gusta el fútbol y a la(0.589)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y a la playa(0.982)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y a la playa también(0.883)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y a la playa también [UNK](0.296)\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol y a la playa también [UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-PZrYYArtMc"
      },
      "source": [
        "Otra posible optimización es lo que se denomina Beam Search. Se deja a modo de ejercicio para entenderlo y una referencia explicativa:\n",
        "https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzKUaOHortMc"
      },
      "source": [
        "## Beam Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDpeeJ1ArtMc"
      },
      "source": [
        "This is a very basic implementation of beam search. I tried to make it readable and understandable, but it's definitely not optimized for speed! The function first uses the model to find the top _k_ words to start the translations (where _k_ is the beam width). For each of the top _k_ translations, it evaluates the conditional probabilities of all possible words it could add to that translation. These extended translations and their probabilities are added to the list of candidates. Once we've gone through all top _k_ translations and all words that could complete them, we keep only the top _k_ candidates with the highest probability, and we iterate over and over until they all finish with an EOS token. The top translation is then returned (after removing its EOS token).\n",
        "\n",
        "* Note: If p(S) is the probability of sentence S, and p(W|S) is the conditional probability of the word W given that the translation starts with S, then the probability of the sentence S' = concat(S, W) is p(S') = p(S) * p(W|S). As we add more words, the probability gets smaller and smaller. To avoid the risk of it getting too small, which could cause floating point precision errors, the function keeps track of log probabilities instead of probabilities: recall that log(a\\*b) = log(a) + log(b), therefore log(p(S')) = log(p(S)) + log(p(W|S))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mi1xCiRf_cif"
      },
      "outputs": [],
      "source": [
        "# extra code – a basic implementation of beam search\n",
        "\n",
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = np.array([sentence_en]).astype(object)  # encoder input\n",
        "    X_dec = np.array([\"startofseq\"]).astype(object)  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = np.array([sentence_en]).astype(object)  # encoder input\n",
        "            X_dec = np.array([\"startofseq \" + translation]).astype(object)  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "jndVFH6qrtMd",
        "outputId": "174e7e62-2c1b-4515-fcb5-d04c53101815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            " me(0.99)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan(0.958)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me encantan los(0.808)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan los perros(0.92)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan los perros y(0.914)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan los perros y a(0.516)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan los perros y a mí(0.468)\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me encantan los perros y a mí'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# extra code – shows how the model making an error\n",
        "sentence_en = \"I love cats and dogs\"\n",
        "translate(sentence_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgMfG50S9p9A"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly half an hour using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "7eKZpXLortMd"
      },
      "outputs": [],
      "source": [
        "# # extra code – shows how beam search can help\n",
        "# beam_search(sentence_en, beam_width=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZA7cMKrtMd"
      },
      "source": [
        "The correct translation is in the top 3 sentences found by beam search, but it's not the first. Since we're using a small vocabulary, the \\[UNK] token is quite frequent, so you may want to penalize it (e.g., divide its probability by 2 in the beam search function): this will discourage beam search from using it too much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNF-QP37rtMd"
      },
      "source": [
        "## Mecanismos de Atencion (Attention mechanisms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52TxgFCRrtMd"
      },
      "source": [
        "Como mejora a este tipo de arquitecturas, en 2014 (Dzmitry Bahdanau y colegas, et al. que se dice) introdujeron una mejora sustancial a la arquitectura de Encoder-Decoders.\n",
        "\n",
        "La idea detrás del mecanismo es pasarle al decoder más información de la secuencia de entrada y no sólo los estados ocultos producidos por el último elemento (el primero y el úlitmo en el caso de bidireccionales). ¿Qué información? Pues algo así como la palabra que más le aporte en cada momento. Por ejemplo que cuando al decoder le toque producir fútbol en la traducción de I like soccer, reciba \"soccer\" (en concreto la salida del encoder a la palabra \"soccer\").  \n",
        "\n",
        "Supongamos que estamos traduciendo frases como:\n",
        "\n",
        "- I like soccer    \n",
        "- I like Rain Man  \n",
        "- you like The Bridge  \n",
        "- we like Jaime  \n",
        "  \n",
        "Para los dos primeras el decoder iría traduciendo:\n",
        "(Me) gusta ... y la idea es que las entradas \"soccer\", \"Rain\" + \"Man\", \"The\" + \"Bridge\", \"Jaime\" aporten más en ese instante...  \n",
        "\n",
        "Entonces al decoder tendré que pasarle todas las palabras de la frase (en concreto la salida de cada una de estas del encoder) y que exista un mecanismo que le diga en función de lo que lleva cuál de las entradas debe considerar más (aquí nos fijamos en la siguiente, pero para traducir \"Me\" es mejor que se fije en la primera, el pronombre, para traducir \"gusta\", igual)  \n",
        "\n",
        "Lo interesante no es considerar solo una palabra de entrada, sino una combinación pesada de estas (una combinación lineal -> \"The\" + \"Bridge\")\n",
        "\n",
        "Es decir, volviendo a nuestras entradas del decoder:\n",
        "\n",
        "* Antes d1: [h(e2),c(2),emb(\"<start_of_sequence>\")], ahora da1 (la a es de attention): [h(e2),c(e2), (coef1 * e1 + coef2 * e2 + coef3 * e3)] y probablemente todos los coef se aproximarían a cero\n",
        "* Antes d2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1)], ahora da2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1), (coef21 * e1 + coef22 * e2 + coef23 * e3)] y probablemente coef21 >> coef22 y coef23\n",
        "* Antes d3: [emb(\"gusta\"),$h_d$(d2),$c_d$(d2)], ahora da3: [emb(\"gusta\"),$h_d$(d1),$c_d$(d1), (coef31 * e1 + coef32 * e2 + coef33 * e3) ] y coef31 y coef32 serán altos y coef33 bajo o nulo\n",
        "\n",
        "\n",
        "__¿Y cómo le digo en cuál debe fijarse más? Pues como en los embeddings... que lo aprenda :-) (o sea tendré una Attention Layer)__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSJSezrertMd"
      },
      "source": [
        "Muy bien, y cómo se hace ese \"que lo aprenda\": Dos mecanismos de Atencion (aditiva y multiplicativa), pero el multiplicativo ha superado al aditivo y de hecho la Attention Layer de keras hace Attention multiplicativa y además al final la predicción se hace a partir de la salida de la capa de atención.\n",
        "\n",
        "Gráficamente:\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder_with_attention.jpg?raw=1\" alt=\"Encoder_Decoder with Attention\" width=\"700\" />\n",
        "\n",
        "\n",
        "Intuitivamente al poner la capa de atención al final está configurando toda la red (toda incluidos los embeddings) para que \"memorice\" la relación estadística entre las posiciones de salida y las de entrada en diferentes situaciones. Y luego ya nosotros a eso le llamamos atención, porque es verdad que cuando llega el momento de \"Me gusta el...\", ha memorizado que las posiciones que deben aportar más es donde haya salidas que generalmente pertenecen a nombres. (pero él ni sabe que son nombres, ni que está traduciendo ni nada,...)  \n",
        "\n",
        "\n",
        "El siguiente paso sería aumentar la memoria y olvidarse de las recurrencias (y del problema de traducir) :-)... los Transformers, pero antes... prestémosle Atención al traductor con atención"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6sxAEvslrtMd"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)) # Ahora necesitamos todas las salidas del Encoder por eso return_sequences = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "SL9ZaWurrtMe"
      },
      "outputs": [],
      "source": [
        "# extra code – this part of the model is exactly the same as earlier\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "# Wrap tf.concat within a Lambda layer to make it compatible with KerasTensors\n",
        "encoder_state = [tf.keras.layers.Lambda(lambda x: tf.concat(x[::2], axis=-1))(encoder_state),  # short-term (0 & 2)\n",
        "                 tf.keras.layers.Lambda(lambda x: tf.concat(x[1::2], axis=-1))(encoder_state)]  # long-term (1 & 3)\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5v4cs87rtMe"
      },
      "source": [
        "And finally, let's add the `Attention` layer and the output layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "6CKhAJ5wgEvW"
      },
      "outputs": [],
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncoE2lSKrtMe"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aICWvCoBrtMe",
        "outputId": "8c0b008e-6094-4f78-a895-da6c92cc5c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 94s 25ms/step - loss: 2.8555 - accuracy: 0.4689 - val_loss: 1.8537 - val_accuracy: 0.5971\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 59s 19ms/step - loss: 1.4961 - accuracy: 0.6514 - val_loss: 1.5243 - val_accuracy: 0.6535\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 59s 19ms/step - loss: 1.1997 - accuracy: 0.7046 - val_loss: 1.4354 - val_accuracy: 0.6743\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 59s 19ms/step - loss: 1.0243 - accuracy: 0.7379 - val_loss: 1.4210 - val_accuracy: 0.6819\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 59s 19ms/step - loss: 0.8921 - accuracy: 0.7646 - val_loss: 1.4316 - val_accuracy: 0.6867\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 59s 19ms/step - loss: 0.7896 - accuracy: 0.7861 - val_loss: 1.4560 - val_accuracy: 0.6870\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 59s 19ms/step - loss: 0.7062 - accuracy: 0.8044 - val_loss: 1.4997 - val_accuracy: 0.6862\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 58s 19ms/step - loss: 0.6366 - accuracy: 0.8203 - val_loss: 1.5411 - val_accuracy: 0.6878\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 59s 19ms/step - loss: 0.5803 - accuracy: 0.8340 - val_loss: 1.5779 - val_accuracy: 0.6860\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 58s 19ms/step - loss: 0.5317 - accuracy: 0.8464 - val_loss: 1.6351 - val_accuracy: 0.6857\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x79671f8690d0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "0gnOroYdrtMe",
        "outputId": "3af509e0-0dd8-4650-9bbb-c0975cd792e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            " me(0.992)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " me gustan(0.62)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gustan tus(0.706)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " me gustan tus zapatos(0.948)\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gustan tus zapatos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "translate(\"I like your shoes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "q8upfzZJ3Bsc",
        "outputId": "e3feff4e-902a-4736-99ec-e1fdf4c42637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            " te(0.746)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " te quiero(0.668)\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'te quiero'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "translate(\"I love you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "ecVI36mR9zR2",
        "outputId": "2618b83b-7d94-439c-c6e1-013ea54fa998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            " me(0.77)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta(0.994)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta la(0.408)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta la fútbol(0.737)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta la fútbol y(0.994)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " me gusta la fútbol y también(0.786)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " me gusta la fútbol y también ir(0.437)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta la fútbol y también ir a(0.963)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta la fútbol y también ir a la(1.0)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta la fútbol y también ir a la playa(0.996)\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta la fútbol y también ir a la playa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DowPj8m1-NiO"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly half an hour using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-rIi2lK7rtMe"
      },
      "outputs": [],
      "source": [
        "# beam_search(\"I like soccer and also going to the beach\", beam_width=3,\n",
        "#             verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx3jPpJOrtMf"
      },
      "source": [
        "## Y si la solución es simple y llanamente establecer relación los elementos de la secuencia entre sí... ***Attention Is All You Need: The Transformer Architecture***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eghXhvLprtMf"
      },
      "source": [
        "Y en 2017, alguien de una empresa..., publicó un paper en el que se presentaba en sociedad una arquitectura que sólo necesitaba de mecanismos de Atenttion y capas Densas para hacer NMT como el mejor.\n",
        "\n",
        "OJO, esta sección es a título ilustrativo... Pero molón\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/transformers.webp?raw=1\" alt=\"Encoder_Decoder with Attention\" width=\"700\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snQivuXHrtMf"
      },
      "source": [
        "Antes de asustarse, lo que realmente hace esta arquitectura es ser la \"superconvolucional\" con memoria de las secuencias de texto, pero vamos por partes:\n",
        "\n",
        "__La parte del encoder__ al incluir el mecanismo de atención (y ese positional encoding) lo que está haciendo es aprender las relaciones posicionales de las palabras del \"lenguaje\" de entrada (sí está memorizando y caracterizando las relaciones entre todas las palabras y sus posiciones de entrada), está haciendo un embedding de la información posicional. Luego concatena la información posicional con la de entrada y aprende a relacionarla lo mejor posible para esa secuencia. En tiempo de inferencia diríamos que para cada secuencia está analizándola sintácticamente, gramaticalmente, etc, etc y luego la devuelve... Pero nunca le hemos pasado información ni sintáctica, ni gramatical, ni nada.  \n",
        "\n",
        "__La parte del decoder__ primero hace lo mismo que el encoder pero con el \"lenguaje\" de destino, se aprende y caracteriza todas las relaciones posicionales de las sentencias de ese \"lenguaje\". Combina ese embedding por secuencia con la secuencia de destino y se lo pasa a la siguiente capa de atención que ahora memoriza y caracteriza todas las relaciones posicionales entre las sentencias procesadas y enriquecidas del lenguaje destino y las sentencias procesadas y enriquecidas del lenguaje de origen. Y luego el feedfoward es el que realmente mezcla todo (mezclará toda la información de la secuencia, vease que se va transmitiendo, las posiciones y las posiciones con el lenguaje origen).\n",
        "\n",
        "Para aumentar la memoria y dar más relaciones lo que hacemos es que la capa de atención en realidad son muchas capas de atención en paralelo y acumular modulos de atención (como acumulábamos capas convolucionales en una red convolucional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl91Sdu4rtMf"
      },
      "source": [
        "A título ilustrativo un __capa multihead de atención__:\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/multihead_attention.png?raw=1\" alt=\"Multihead Attention Layer\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh_6C3mWrtMf"
      },
      "source": [
        "Las capas linear son capas densas sin función de activación, tienen pesos entrenables -> Memoria (en estos pesos está la memoria de las características posicionales)\n",
        "Y las capas head fuerzan que las linear (tantas como cabezas*3) aprendan un número de relaciones \"poscionales\" que dependen del número de cabezas (si quiero que apredan más relaciones posicionales -> Más cabezas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zna3md12rtMf"
      },
      "source": [
        "Y para terminar hay N modulos (en el paper de 2017, 6 por Encoder y 6 por Decoder) apilados, ¿por qué? Aquí una idea intuitiva es la misma que en las convolucionales, para que pueda memorizar relaciones posicionales más complicadas (y tener más memoria entrenable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3ar9UqqrtMf"
      },
      "source": [
        "## Large Language Models (Pretrained Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt8wIErgrtMf"
      },
      "source": [
        "Y en 2018, llegaron las arquitecturas que apilaban multihead attention layers pero ya olvidándose de decoders y encoders... Una sola columna con muchos módulos... pero con la magia de estar pre-entrenadas para una tarea (generalmente adivina cuál es la siguiente palabra de la sentencia (GPT) o de cada sentencia te voy a ocultar 2 palabras, adivina cuales son (BERT)).\n",
        "\n",
        "En esencia, están memorizando todas las relaciones que existen entre las palabras de un lenguaje... Por eso cada vez se hacen más grandes (para memorizar más) y tienen más parámetros... y necesitan más que les des de comer (si quieres tenerlo todo representado)... (Imagina que le pudieras dar para entrenar a un red convolucional todas las imágenes posibles que existen)\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/gpt2_vs_BERT.jpg?raw=1\" alt=\"Multihead Attention Layer\" width=\"800\" />\n",
        "\n",
        "Luego estos modelos preentrenados se adaptan (fine-tunning), o sea se hace transfer learning, a otros tipos de problemas (clasificación, sentiment analysis, question and answers, y ahora chats, texto generativo).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOi7vkFOAClz"
      },
      "source": [
        "## Instruct LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_8qCUXkAE_r"
      },
      "source": [
        "Hoy en día, lo que realmente se ha puesto \"de moda\" no es emplear los LLM como los vistos hasta ahora, sino los fine-tuned y más concretamente los basados en una aproximación denominada \"Instruct LLM\" (partiendo de un paper de OpenAI):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yY38EYRAbDw"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/rlhf.jpg?raw=1\" alt=\"Instruct LLM\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrzskW7CrtMg"
      },
      "source": [
        "Han surgido decenas de modelos, propietarios y abiertos, los más destacados (que puedes usar):  \n",
        "[OpenAI: ChatGPT](https://chat.openai.com/)  \n",
        "[Google: Gemini](https://gemini.google.com/?hl=es)  \n",
        "[Mistral: Mistral MoE](https://mistral.ai/)  \n",
        "[Meta: Llama](https://www.llama.com/)  \n",
        "[Hugging Face: miles y miles de modelos](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)  \n",
        "[Preplexity AI: combina GPT-3.5 y su LLM propietario](https://www.perplexity.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qhj4QK6rtMg"
      },
      "source": [
        "### Bonus: PROGRAMANDO UN TRANSFORMER (GPT-2 like)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZwnnhSVrtMh"
      },
      "source": [
        "En las siguientes celdas se muestra como construir un transformer para nuestro traductor... para el que quiera jugar (extraido del Hands-On...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5UExD9rtMh"
      },
      "source": [
        "Positional encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrOa1E-Qx7la"
      },
      "outputs": [],
      "source": [
        "tf.shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2TZgJSZrtMi"
      },
      "outputs": [],
      "source": [
        "max_length = 50  # max length in the whole training set\n",
        "embed_size = 128\n",
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
        "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
        "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
        "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwLLeq-TrtMi"
      },
      "source": [
        "Alternatively, we can use fixed, non-trainable positional encodings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hjsMZ3xrtMi"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length),\n",
        "                           2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vArtPEfzrtMi"
      },
      "outputs": [],
      "source": [
        "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
        "encoder_in = pos_embed_layer(encoder_embeddings)\n",
        "decoder_in = pos_embed_layer(decoder_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkyBTS9HrtMi"
      },
      "outputs": [],
      "source": [
        "# extra code – this cells generates and saves Figure 16–9\n",
        "figure_max_length = 201\n",
        "figure_embed_size = 512\n",
        "pos_emb = PositionalEncoding(figure_max_length, figure_embed_size)\n",
        "zeros = np.zeros((1, figure_max_length, figure_embed_size), np.float32)\n",
        "P = pos_emb(zeros)[0].numpy()\n",
        "i1, i2, crop_i = 100, 101, 150\n",
        "p1, p2, p3 = 22, 60, 35\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
        "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
        "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
        "ax1.plot(p3, P[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
        "ax1.plot(P[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
        "ax1.plot(P[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
        "ax1.plot([p1, p2], [P[p1, i1], P[p2, i1]], \"bo\")\n",
        "ax1.plot([p1, p2], [P[p1, i2], P[p2, i2]], \"ro\")\n",
        "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
        "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.hlines(0, 0, figure_max_length - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
        "ax1.axis([0, figure_max_length - 1, -1, 1])\n",
        "ax2.imshow(P.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
        "ax2.hlines(i1, 0, figure_max_length - 1, color=\"b\", linewidth=3)\n",
        "cheat = 2  # need to raise the red line a bit, or else it hides the blue one\n",
        "ax2.hlines(i2+cheat, 0, figure_max_length - 1, color=\"r\", linewidth=3)\n",
        "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
        "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
        "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
        "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
        "ax2.axis([0, figure_max_length - 1, 0, crop_i])\n",
        "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
        "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
        "save_fig(\"positional_embedding_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP0A5FmgrtMj"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaNUlZBNrtMj"
      },
      "outputs": [],
      "source": [
        "N = 2  # instead of 6\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = 128  # for the first Dense layer in each Feed Forward block\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
        "Z = encoder_in\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZtmcl8lrtMj"
      },
      "outputs": [],
      "source": [
        "encoder_outputs = Z  # let's save the encoder's final outputs\n",
        "Z = decoder_in  # the decoder starts with its own inputs\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQW4XffvrtMj"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly 2 or 3 hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmC-BfkgrtMj"
      },
      "outputs": [],
      "source": [
        "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThoiaaMvrtMj"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXaMFZc8rtMj"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}